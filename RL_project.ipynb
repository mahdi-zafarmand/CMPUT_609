{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Definition\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "\n",
    "In this project I am going to implement __q-sigma__ method and compare it with __SARSA__ and __TreeBackUp__.\n",
    "My initial hypothesis is the __q-sigma__ will perform better than both __SARSA__ and __TreeBackUp__ since it will benefit from both methods. I am going to test my hypothesis using the __Grid World__ problem, which is a well-known problem described in the book. In this problem the objective is reaching to the __goal__ state, starting from __start__ state, considering there are multiple __obstacles__ in the way, so the agent should avoid these obstacles.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Environment of the Problem\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "\n",
    "The first class that I implemented in this project is Environment, which represents the envorinment the agent works in. In the array below, __S__ is used to indicate where the starting point is, __G__ shows the goal state, __X__ signs represent obstacles of the environment and the __#__ sign shows a checkpoint state inside the world.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.info = {}\n",
    "        self.states = []\n",
    "        self.rewards = {}\n",
    "        self.initialize()\n",
    "\n",
    "    # this method initializes the attributes of the Environment class.\n",
    "    def initialize(self):\n",
    "        self.reset()\n",
    "        self.set_states()\n",
    "        self.set_rewards()\n",
    "\n",
    "    # this method resets the environment, everytime the agent coincides with an obstacle.\n",
    "    def reset(self):\n",
    "        grid = np.array([\n",
    "            [\"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"G\"],\n",
    "            [\"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\"],\n",
    "            [\"_\", \"X\", \"X\", \"X\", \"X\", \"X\", \"X\", \"X\", \"X\", \"X\"],\n",
    "            [\"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\"],\n",
    "            [\"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\"],\n",
    "            [\"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\"],\n",
    "            [\"X\", \"X\", \"X\", \"X\", \"X\", \"X\", \"_\", \"_\", \"_\", \"_\"],\n",
    "            [\"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\"],\n",
    "            [\"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"#\", \"_\"],\n",
    "            [\"S\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\", \"_\"]\n",
    "        ])\n",
    "        self.info['grid'] = grid\n",
    "        self.info['size'] = grid.shape[0]\n",
    "        self.info['borders'] = {}\n",
    "        self.info['borders']['>'] = range(grid.shape[0]-1, grid.shape[0]**2, grid.shape[0])\n",
    "        self.info['borders']['<'] = range(0, grid.shape[0]**2, grid.shape[0])\n",
    "        self.info['borders']['^'] = range(grid.shape[0])\n",
    "        self.info['borders']['v'] = range(grid.shape[0] * (grid.shape[0] - 1), grid.shape[0]**2)\n",
    "        self.info['obstacles'] = [21, 22, 23, 24, 25, 26, 27, 28, 29, 60, 61, 62, 63, 64, 65]\n",
    "        self.info['start'] = 90\n",
    "        self.info['goal'] = 9\n",
    "        self.info['checkpoint'] = 88\n",
    "\n",
    "    # this method defines the state number for each state.\n",
    "    def set_states(self):\n",
    "        size_of_environment = self.info['grid'].shape\n",
    "        for i in range(size_of_environment[0] * size_of_environment[1]):\n",
    "            self.states.append((i // size_of_environment[0], i % size_of_environment[1]))\n",
    "\n",
    "    # this method determines how many reward units the agent would receive after meeting different state types. \n",
    "    def set_rewards(self):\n",
    "        self.rewards['crash'] = -10\n",
    "        self.rewards['checkpoint'] = 0\n",
    "        self.rewards['win'] = 1000\n",
    "        self.rewards['step'] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Agent of the Problem\n",
    "\n",
    "\n",
    "After defining the environment, it is time to define our __agent__ that is going to walk in the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, environment):\n",
    "        self.environment = environment\n",
    "        self.states = environment.states\n",
    "        self.actions = {}\n",
    "        self.sigma = 0.5\n",
    "        self.set_actions()\n",
    "        self.P = np.zeros((len(self.states), self.actions['number']))\n",
    "        self.Q = np.zeros(self.P.shape)\n",
    "\n",
    "\t# this method determines valid moves for the agent.\n",
    "    def set_actions(self):\n",
    "        self.actions['RIGHT'] = 0\n",
    "        self.actions['UP'] = 1\n",
    "        self.actions['LEFT'] = 2\n",
    "        self.actions['DOWN'] = 3\n",
    "        self.actions['number'] = len(self.actions)\n",
    "\n",
    "\t# this method resets the probability matrix and the state-action matrix.\n",
    "    def reset_p_and_q(self):\n",
    "        self.P = np.zeros((len(self.states), self.actions['number']))\n",
    "        self.Q = np.zeros(self.P.shape)        \n",
    "\n",
    "\t# this method forces the agent to act greedily w.r.t epsilon.\n",
    "    def make_greedy(self, s, epsilon):\n",
    "        self.P[s, :] = [epsilon / (self.actions['number'] - 1)]\n",
    "        best_action = np.argmax(self.Q[s, :])\n",
    "        self.P[s, best_action] = 1 - epsilon\n",
    "\n",
    "\t# this method chooses the action for the agent based on the greedy behavior of the agent.\n",
    "    def choose_action(self, s, epsilon):\n",
    "        self.make_greedy(s, epsilon)\n",
    "        return np.random.choice(range(self.actions['number']), p=self.P[s, :])\n",
    "\n",
    "\t# this method chooses the sigma which is the key argument in the q-sigma algorithm, it is fixed for SARSA and TreeBackUp.\n",
    "    def choose_sigma(self, alg, mode):\n",
    "        if alg == 'SARSA':\n",
    "            return 1\n",
    "        elif alg == 'TreeBackUp':\n",
    "            return 0\n",
    "        elif alg == 'Qsigma' and mode == 'random':\n",
    "            return int(np.random.random() < self.sigma)\n",
    "        elif alg == 'Qsigma' and mode == 'increasing':\n",
    "            return None\n",
    "        elif alg == 'Qsigma' and mode == 'decreasing':\n",
    "            return None\n",
    "        print('ERROR, incorrect sigma alg!')\n",
    "        return None\n",
    "\n",
    "\t# this method moves the agent to the next state and returns the next state and the corresponding reward.\n",
    "    def move(self, s, a):\n",
    "        if a == 0:\n",
    "            return self.move_helper(s, s + 1, '>')\n",
    "        elif a == 1:\n",
    "            return self.move_helper(s, s - self.environment.info['size'], '^')\n",
    "        elif a == 2:\n",
    "            return self.move_helper(s, s - 1, '<')\n",
    "        elif a == 3:\n",
    "            return self.move_helper(s, s + self.environment.info['size'], 'v')\n",
    "        print('ERROR, incorrect action!')\n",
    "        return None\n",
    "\n",
    "\t# this method is an  auxialary method that helps 'move' method to function.\n",
    "    def move_helper(self, s_prev, s_next, direction):\n",
    "        self.environment.info['grid'][self.states[s_prev]] = direction\n",
    "        border = self.environment.info['borders'][direction]\n",
    "        if s_prev in border or s_next in self.environment.info['obstacles']:\n",
    "            self.environment.reset()\n",
    "            return self.environment.info['start'], self.environment.rewards['crash']\n",
    "        self.environment.info['grid'][self.states[s_next]] = 'O'\n",
    "        if s_next == self.environment.info['checkpoint']:\n",
    "            return s_next, self.environment.rewards['checkpoint']\n",
    "        elif s_next == self.environment.info['goal']:\n",
    "            return s_next, self.environment.rewards['win']\n",
    "        else:\n",
    "            return s_next, self.environment.rewards['step']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining an Episode to Solve the Problem\n",
    "\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "\n",
    "Now we have our agent, it is time to define the problem in its simplest form, __an episode__.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episode:\n",
    "    def __init__(self, environment, agent):\n",
    "        self.cum_steps = 0\n",
    "        self.cum_reward = 0\n",
    "        self.states_list = []\n",
    "        self.actions = []\n",
    "        self.p = []\n",
    "        self.q = []\n",
    "        self.rewards_list = []\n",
    "        self.environment = environment\n",
    "        self.agent = agent\n",
    "        self.t = -1\n",
    "        self.T = np.inf\n",
    "        self.tau = 0\n",
    "        self.s_next = 0\n",
    "        self.r = 0\n",
    "        self.a_next = 0\n",
    "        self.sigma = 0\n",
    "        self.sigmas = []\n",
    "\n",
    "\t# this method initializes the Episode class then makes the agent choose an action.\n",
    "    def initialize(self, args):\n",
    "        action = self.agent.choose_action(self.environment.info['start'], args.epsilon)\n",
    "        self.states_list.append(self.environment.info['start'])\n",
    "        self.actions.append(action)\n",
    "        self.sigmas = [1]\n",
    "        self.p.append(self.agent.P[self.environment.info['start'], action])\n",
    "        self.q.append(self.agent.Q[self.environment.info['start'], action])\n",
    "\n",
    "\t# this method updates the state of the agent and the received reward.\n",
    "    def update_next_state_and_reward(self):\n",
    "        self.s_next, self.r = self.agent.move(self.states_list[self.t], self.actions[self.t])\n",
    "        self.states_list.append(self.s_next)\n",
    "        self.cum_steps += 1\n",
    "        self.cum_reward += self.r\n",
    "\n",
    "\t# this method updates the action of the agent and the working sigma.\n",
    "    def update_next_action_and_sigma(self, args):\n",
    "        self.a_next = self.agent.choose_action(self.states_list[self.t+ 1], args.epsilon)\n",
    "        self.actions.append(self.a_next)\n",
    "        self.sigma = self.agent.choose_sigma(args.alg, args.mode)\n",
    "        self.sigmas.append(self.sigma)\n",
    "\n",
    "\t# this method calculates the target value.\n",
    "    def calc_target(self, args):\n",
    "        target = self.r + self.sigma * args.gamma * self.q[self.t + 1]\n",
    "        target += (1 - self.sigma) * args.gamma * np.dot(self.agent.P[self.s_next, :], self.agent.Q[self.s_next, :])\n",
    "        target -= self.q[self.t]\n",
    "        return target\n",
    "\n",
    "\t# this method updates the q matrix and the number of steps the agent should consider for the overal return value (tau)\n",
    "    def update_q_and_return_tau(self, args):\n",
    "        self.tau = self.t - args.n_step + 1\n",
    "        if self.tau >= 0:\n",
    "            e = 1\n",
    "            g = self.q[self.tau]\n",
    "            for k in range(self.tau, min(self.t, self.T - 1) + 1):\n",
    "                g += e * self.rewards_list[k]\n",
    "                e *= args.gamma * ((1-self.sigmas[k]) * self.p[k] + self.sigmas[k])\n",
    "            error = g - self.agent.Q[self.states_list[self.tau], self.actions[self.tau]]\n",
    "            self.agent.Q[self.states_list[self.tau], self.actions[self.tau]] += args.alpha * error\n",
    "            self.agent.make_greedy(self.states_list[self.tau], args.epsilon)\n",
    "\n",
    "\t# this method makes the agent to run for a single episode.\n",
    "    def run(self, args, episode_counter=0, n_steps=[], rewards=[]):\n",
    "        print('\\nEpisode: ' + str(episode_counter + 1) + '/' + str(args.n_episodes) + \" ...\")\n",
    "        self.environment.reset()\n",
    "        self.initialize(args)\n",
    "        while self.tau != self.T - 1:\n",
    "            self.t += 1\n",
    "            if self.t < self.T:\n",
    "                self.update_next_state_and_reward()\n",
    "                if self.s_next == self.environment.info['goal']:\n",
    "                    self.T = self.t + 1\n",
    "                    self.rewards_list.append(self.r - self.q[self.t])\n",
    "                else:\n",
    "                    self.update_next_action_and_sigma(args)\n",
    "                    self.q.append(self.agent.Q[self.s_next, self.a_next])\n",
    "                    target = self.calc_target(args)\n",
    "                    self.rewards_list.append(target)\n",
    "                    self.p.append(self.agent.P[self.s_next, self.a_next])\n",
    "            self.update_q_and_return_tau(args)\n",
    "        print(self.environment.info['grid'])\n",
    "        print('number of steps = ' + str(self.cum_steps))\n",
    "        n_steps.append(self.cum_steps)\n",
    "        rewards.append(self.cum_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running an Episode\n",
    "\n",
    "\n",
    "In an episode the agent finally reaches the __goal__ state. I anticipate if the agent is starting with an initially random P, the number of steps it takes for the agent to meet the goal state is relatively high.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self):\n",
    "        self.alg = 'SARSA'\n",
    "        self.mode = 'random'\n",
    "        self.n_episodes = 2000\n",
    "        self.n_step = 5\n",
    "        self.gamma = 0.99\n",
    "        self.alpha = 0.1\n",
    "        self.epsilon = 0.1\n",
    "        self.n_experiments = 10\n",
    "        self.n_steps = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def reset(self):\n",
    "        self.n_steps = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def set_n_experiments(self, n):\n",
    "        self.n_experiments = n\n",
    "\n",
    "    def set_n_episodes(self, n):\n",
    "        self.n_episodes = n\n",
    "        \n",
    "    def run(self, environment, agent, averages):\n",
    "        agent.reset_p_and_q()\n",
    "        for episode_counter in range(self.n_episodes):\n",
    "            episode = Episode(environment, agent)\n",
    "            episode.run(self, episode_counter, self.n_steps, self.rewards)\n",
    "        \n",
    "        averages['average_steps'].append(np.average(self.n_steps))\n",
    "        if len(averages['average_steps']) > 1:\n",
    "            print('average number of steps = ' + str(averages['average_steps'][-1]))\n",
    "        averages['average_reward'].append(np.average(self.rewards))\n",
    "        print('average return = ' + str(averages['average_reward'][-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Project Class\n",
    "\n",
    "The project class initializes the number of experiments and number of episodes that you want to run, then it runs the experiment's 'run' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Project:\n",
    "    def __init__(self):\n",
    "        self.environment = Environment()\n",
    "        self.agent = Agent(self.environment)\n",
    "        self.experiment = Experiment()\n",
    "        self.averages = {}\n",
    "\n",
    "    def initialize(self, n_experiments, n_episodes):\n",
    "        self.averages['average_steps'] = []\n",
    "        self.averages['average_reward'] = []\n",
    "        self.experiment.set_n_experiments(n_experiments)\n",
    "        self.experiment.set_n_episodes(n_episodes)\n",
    "\n",
    "    def run(self):\n",
    "        for _ in range(self.experiment.n_experiments):\n",
    "            self.experiment.reset()\n",
    "            self.experiment.run(self.environment, self.agent, self.averages)\n",
    "\n",
    "    def print_results(self, detail = None):\n",
    "        if detail:\n",
    "            print(\"\\nsteps: \" + str(self.averages['average_steps']))\n",
    "            print(\"steps avg: \" + str(np.average(self.averages['average_steps'])))\n",
    "            print(\"rewards: \" + str(self.averages['average_reward']))\n",
    "            print(\"rewards avg: \" + str(np.average(self.averages['average_reward'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Project\n",
    "\n",
    "In this part, the only thing that is necessary to do is to instantiate a project, then initialize it, which means you have to determine how many episodes and experiments you want to have in total, then run the project and see the results if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode: 1/1 ...\n",
      "[['>' '>' '>' '>' '>' '>' '>' '>' '>' 'O']\n",
      " ['^' '_' '_' '_' '_' '_' '_' '_' '_' '_']\n",
      " ['^' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']\n",
      " ['^' '<' '<' 'v' '<' 'v' '_' '_' '_' '_']\n",
      " ['_' '_' '^' '<' '^' '<' '<' '_' '_' '_']\n",
      " ['_' '_' '_' '_' '^' '<' '<' '_' '_' '_']\n",
      " ['X' 'X' 'X' 'X' 'X' 'X' '^' '_' '_' '_']\n",
      " ['_' '_' '_' '_' '>' '>' '^' '_' '_' '_']\n",
      " ['>' '>' '>' '>' '^' '_' '_' '_' '#' '_']\n",
      " ['^' '_' '_' '_' '_' '_' '_' '_' '_' '_']]\n",
      "number of steps = 19189\n",
      "average return = -25074.0\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    n_experiments = 1\n",
    "    n_episodes = 1\n",
    "    project = Project()\n",
    "    project.initialize(n_experiments, n_episodes)\n",
    "    project.run()\n",
    "    project.print_results()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
